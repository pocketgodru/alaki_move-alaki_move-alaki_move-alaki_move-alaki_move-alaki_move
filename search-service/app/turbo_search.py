import numpy as np
from pymongo import MongoClient
from sentence_transformers import SentenceTransformer
from sklearn.preprocessing import normalize
import faiss
import torch
import hashlib
import os
import re
from time import time
from typing import List, Dict, Any

class TurboMovieSearch:
    def __init__(self, mongo_host="mongodb://mongodb:27017", mongo_db="movies_db", mongo_collection="movies"):
        print("üöÄ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ–∏—Å–∫–æ–≤–æ–π —Å–∏—Å—Ç–µ–º—ã...")
        self.client = MongoClient(mongo_host)
        self.db = self.client[mongo_db]
        self.collection = self.db[mongo_collection]

        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ MongoDB
        self.metadata = self._load_metadata()
        self.embeddings = self._load_or_generate_embeddings()

        # FAISS Index
        self.index = faiss.IndexFlatL2(self.embeddings.shape[1])
        self.index.add(self.embeddings)

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –º–æ–¥–µ–ª–∏
        device = "cuda" if torch.cuda.is_available() else "mps" if torch.backends.mps.is_available() else "cpu"
        print(f"üñ• –ò—Å–ø–æ–ª—å–∑—É–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")
        
        self.model = SentenceTransformer(
            'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2',
            device=device,
            cache_folder='model_cache'
        )

        # –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã–π —Ä–∞—Å—á—ë—Ç –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø–æ –∂–∞–Ω—Ä–∞–º –∏ –≥–æ–¥–∞–º
        self._precompute_features()
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫—ç—à–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞
        self.search_cache = {}
        self.cache_hits = 0
        self.total_searches = 0
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–∏–ª—å–º–æ–≤ –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –∏–∑–º–µ–Ω–µ–Ω–∏–π
        self.movie_count = len(self.metadata)
        
        print("‚úÖ –ü–æ–∏—Å–∫–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –≥–æ—Ç–æ–≤–∞ –∫ —Ä–∞–±–æ—Ç–µ!")

    def _load_metadata(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Ñ–∏–ª—å–º—ã –∏–∑ MongoDB"""
        movies = list(self.collection.find({}, {"_id": 0}))
        print(f"üì• –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(movies)} —Ñ–∏–ª—å–º–æ–≤ –∏–∑ MongoDB")
        return movies

    def _load_or_generate_embeddings(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏"""
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –≤–æ–∑–º–æ–∂–Ω—ã—Ö –ø—É—Ç–µ–π –∫ —Ñ–∞–π–ª—É
            possible_paths = [
                "/app/movies_embeddings.npy",  # –ú–æ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ñ–∞–π–ª
                "movies_embeddings.npy",  # –í —Ç–µ–∫—É—â–µ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
                "../movies_embeddings.npy",  # –ù–∞ —É—Ä–æ–≤–µ–Ω—å –≤—ã—à–µ
            ]
            
            for path in possible_paths:
                try:
                    print(f"üîç –ü—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –∏–∑ {path}...")
                    embeddings = np.load(path)
                    print(f"‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏–∑ {path}")
                    print(f"üìä –†–∞–∑–º–µ—Ä —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {embeddings.shape}")
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –ª–∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É —Ñ–∏–ª—å–º–æ–≤
                    if len(embeddings) == len(self.metadata):
                        return embeddings
                    else:
                        print(f"‚ö†Ô∏è –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ ({len(embeddings)}) –Ω–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤—É —Ñ–∏–ª—å–º–æ–≤ ({len(self.metadata)})")
                        continue
                        
                except (FileNotFoundError, PermissionError) as e:
                    print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏–∑ {path}: {str(e)}")
                    continue
            
            print("‚ùå –§–∞–π–ª —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω –∏–ª–∏ –Ω–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤—É —Ñ–∏–ª—å–º–æ–≤")
            raise FileNotFoundError("–§–∞–π–ª —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω –∏–ª–∏ –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–µ–Ω")
                
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {str(e)}")
            raise Exception("–ù–µ–≤–æ–∑–º–æ–∂–Ω–æ –∑–∞–≥—Ä—É–∑–∏—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∏")

    def _precompute_features(self):
        """–ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –≤—ã—á–∏—Å–ª—è–µ–º –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏"""
        years = np.array([item.get('year', 2000) for item in self.metadata], dtype=np.float32)
        self.norm_years = (years - years.min()) / (years.max() - years.min())

        self.genre_index = {}
        for idx, item in enumerate(self.metadata):
            for genre in item.get('genres', []):
                if genre not in self.genre_index:
                    self.genre_index[genre] = []
                self.genre_index[genre].append(idx)

        self.embeddings = normalize(self.embeddings)
    
    def _get_cache_key(self, query, year_filter, genre_filter):
        """–°–æ–∑–¥–∞–µ—Ç —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –∫–ª—é—á –¥–ª—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞"""
        key = f"{query}|{year_filter}|{genre_filter}"
        return hashlib.md5(key.encode()).hexdigest()

    def _parse_query(self, query: str):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ñ–∏–ª—å—Ç—Ä–æ–≤ –∏–∑ –∑–∞–ø—Ä–æ—Å–∞"""
        year_match = re.search(r'\b(19\d{2}|20[0-2]\d)\b', query)
        year_boost = None
        if year_match:
            year = int(year_match.group())
            year_boost = (year - 1900) / 125

        genres = []
        for genre in self.genre_index.keys():
            if re.search(r'\b' + re.escape(genre) + r'\b', query, re.IGNORECASE):
                genres.append(genre)

        clean_query = re.sub(r'\b\d{4}\b', '', query).strip()
        return clean_query, year_boost, genres

    def search(self, query: str, top_k=10, year_filter=None, genre_filter=None):
        """
        –ü–æ–∏—Å–∫ —Ñ–∏–ª—å–º–æ–≤ –ø–æ –∑–∞–ø—Ä–æ—Å—É —Å —É—á–µ—Ç–æ–º —Ñ–∏–ª—å—Ç—Ä–æ–≤
        """
        start_time = time()
        self.total_searches += 1
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
        cache_key = self._get_cache_key(query, year_filter, genre_filter)
        if cache_key in self.search_cache:
            self.cache_hits += 1
            hit_rate = (self.cache_hits / self.total_searches) * 100
            print(f"üîç –ö—ç—à-—Ö–∏—Ç! ({self.cache_hits}/{self.total_searches}, {hit_rate:.1f}%)")
            return self.search_cache[cache_key]

        clean_query, year_boost, genres = self._parse_query(query)

        if year_filter:
            try:
                year_boost = (int(year_filter) - 1900) / 125
            except (ValueError, TypeError):
                year_boost = None
                
        if genre_filter:
            genres.append(genre_filter.lower())

        # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –∑–∞–ø—Ä–æ—Å–∞
        query_embedding = self.model.encode(
            clean_query,
            convert_to_numpy=True,
            normalize_embeddings=True
        )

        # –í—ã—á–∏—Å–ª—è–µ–º —Ç–µ–∫—Å—Ç–æ–≤–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
        text_scores = np.dot(self.embeddings, query_embedding.T).flatten()
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Å–∫–æ—Ä—ã
        year_scores = np.zeros_like(text_scores)
        genre_scores = np.zeros_like(text_scores)

        # –£—á–∏—Ç—ã–≤–∞–µ–º –≥–æ–¥, –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω
        if year_boost is not None:
            year_scores = 1.0 - np.abs(self.norm_years - year_boost)

        # –£—á–∏—Ç—ã–≤–∞–µ–º –∂–∞–Ω—Ä—ã
        if genres:
            for genre in genres:
                if genre in self.genre_index:
                    genre_scores[self.genre_index[genre]] += 0.1

        # –ö–æ–º–±–∏–Ω–∏—Ä—É–µ–º –≤—Å–µ —Å–∫–æ—Ä—ã —Å –≤–µ—Å–∞–º–∏
        total_scores = 0.85 * text_scores + 0.05 * year_scores + 0.1 * genre_scores

        # –ü–æ–ª—É—á–∞–µ–º —Ç–æ–ø-K —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        indices = np.argpartition(total_scores, -top_k)[-top_k:]
        best_indices = indices[np.argsort(-total_scores[indices])]

        # –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        results = []
        for idx in best_indices:
            if total_scores[idx] > 0.1:  # –§–∏–ª—å—Ç—Ä—É–µ–º –Ω–∏–∑–∫–æ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                movie = self.metadata[idx].copy()
                movie['relevance_score'] = float(total_scores[idx])
                results.append(movie)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à
        self.search_cache[cache_key] = results
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –∫—ç—à–∞
        if len(self.search_cache) > 1000:
            random_key = next(iter(self.search_cache))
            del self.search_cache[random_key]

        print(f"‚è± –ü–æ–∏—Å–∫ –∑–∞ {time() - start_time:.2f}s | –ù–∞–π–¥–µ–Ω–æ {len(results)} —Ñ–∏–ª—å–º–æ–≤")
        return results 